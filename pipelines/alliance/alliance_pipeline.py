import argparse
import concurrent.futures
import datetime
import enum
import json
import logging
import multiprocessing
import os
import threading
import time
import traceback
from string import Template

from genedescriptions.commons import DataType
from genedescriptions.config_parser import GenedescConfigParser
from genedescriptions.descriptions_writer import DescriptionsWriter
from genedescriptions.gene_description import GeneDescription
from genedescriptions.ontology_cache import install_ontology_cache
from genedescriptions.precanned_modules import set_expression_module, set_gene_ontology_module, set_disease_module, \
    set_alliance_human_orthology_module
from pipelines.alliance.alliance_data_manager import AllianceDataManager, provider_to_expression_curie_prefix

logger = logging.getLogger(__name__)

SPECIES_BY_PROVIDER = {
    'WB': 'Caenorhabditis elegans',
    'ZFIN': 'Danio rerio',
    'FB': 'Drosophila melanogaster',
    'HUMAN': 'Homo sapiens',
    'MGI': 'Mus musculus',
    'RGD': 'Rattus norvegicus',
    'SGD': 'Saccharomyces cerevisiae',
    'XBXL': 'Xenopus laevis',
    'XBXT': 'Xenopus tropicalis'
}

TAXON_BY_PROVIDER = {
    'WB': '6239',
    'ZFIN': '7955',
    'FB': '7227',
    'HUMAN': '9606',
    'MGI': '10090',
    'RGD': '10116',
    'SGD': '559292',
    'XBXL': '8355',
    'XBXT': '8364'
}

FILE_HEADER_TEMPLATE = Template("""\
##########################################################################
#
# Data type: $file_type
# Data format: $data_format
# README: $readme
# Source: Alliance of Genome Resources (Alliance)
# Source URL: https://www.alliancegenome.org/downloads
# Help Desk: help@alliancegenome.org
# TaxonIDs: NCBITaxon:$taxon_id
# Species: $species
# Alliance Database Version: $database_version
# Date file generated (UTC): $gen_time
#
##########################################################################""")

FILE_README = (
    "This file contains the following fields: gene ID, gene name, and "
    "gene description. The gene descriptions are generated by an algorithm "
    "developed by the Alliance that uses highly structured gene data such as "
    "associations to various ontology terms (e.g., Gene Ontology terms) and "
    "the Alliance strict orthology set. The original set of ontology terms "
    "that a gene is annotated to may have been trimmed to an ancestor term "
    "in the ontology, in order to balance readability with the amount of "
    "information in the description. The complete set of annotations to any "
    "gene in this file may be found in the relevant data tables on the "
    "Alliance gene page."
)


class ProviderPhase(str, enum.Enum):
    PENDING = "pending"
    LOADING_DATA = "loading_data"
    LOADING_ORTHOLOGS = "loading_ortho"
    PROCESSING = "processing"
    WRITING_JSON = "writing_json"
    WRITING_FILES = "writing_files"
    WRITING_DB = "writing_db"
    DONE = "done"
    ERROR = "error"


class ProgressTracker:
    """Tracks per-provider progress through the pipeline.

    Stores each provider's state as a JSON string in a dict.
    When ``shared=True`` the backing dict is a
    ``multiprocessing.Manager().dict()`` so child processes can
    update it and the main-process StatusLogger sees the changes.
    """

    _PHASE_PCT_RANGES = {
        ProviderPhase.PENDING: (0, 0),
        ProviderPhase.LOADING_DATA: (0, 15),
        ProviderPhase.LOADING_ORTHOLOGS: (15, 25),
        ProviderPhase.PROCESSING: (25, 85),
        ProviderPhase.WRITING_JSON: (85, 92),
        ProviderPhase.WRITING_FILES: (92, 96),
        ProviderPhase.WRITING_DB: (96, 100),
        ProviderPhase.DONE: (100, 100),
        ProviderPhase.ERROR: (0, 0),
    }

    def __init__(self, providers, shared=False):
        if shared:
            self._manager = multiprocessing.Manager()
            self._data = self._manager.dict()
        else:
            self._manager = None
            self._data = {}
        initial = json.dumps({
            "phase": ProviderPhase.PENDING.value,
            "percentage": 0.0,
            "total_genes": 0,
            "genes_done": 0,
            "batch_index": 0,
            "total_batches": 0,
            "detail": "",
            "start_time": None,
            "elapsed": None,
        })
        for provider in providers:
            self._data[provider] = initial

    def _get(self, provider):
        return json.loads(self._data[provider])

    def _put(self, provider, state):
        self._data[provider] = json.dumps(state)

    def set_phase(self, provider, phase, detail="", total_genes=0):
        state = self._get(provider)
        state["phase"] = phase.value
        lo, _ = self._PHASE_PCT_RANGES[phase]
        state["percentage"] = float(lo)
        state["detail"] = detail
        if phase == ProviderPhase.LOADING_DATA and state["start_time"] is None:
            state["start_time"] = time.time()
        if total_genes:
            state["total_genes"] = total_genes
        if phase == ProviderPhase.DONE:
            state["percentage"] = 100.0
            if state["start_time"] is not None:
                state["elapsed"] = time.time() - state["start_time"]
            state["detail"] = "complete"
        if phase == ProviderPhase.ERROR:
            state["percentage"] = 0.0
        self._put(provider, state)

    def update_loading_progress(self, provider, completed, total, current_task=""):
        """Update sub-progress within the LOADING_DATA phase."""
        state = self._get(provider)
        lo, hi = self._PHASE_PCT_RANGES[ProviderPhase.LOADING_DATA]
        fraction = completed / total if total else 0
        state["percentage"] = lo + fraction * (hi - lo)
        state["detail"] = f"{completed}/{total} loads done" + (f", {current_task}" if current_task else "")
        self._put(provider, state)

    def update_processing_progress(self, provider, batch_index, total_batches,
                                   genes_done, total_genes):
        state = self._get(provider)
        state["batch_index"] = batch_index
        state["total_batches"] = total_batches
        state["genes_done"] = genes_done
        state["total_genes"] = total_genes
        lo, hi = self._PHASE_PCT_RANGES[ProviderPhase.PROCESSING]
        fraction = genes_done / total_genes if total_genes else 0
        state["percentage"] = lo + fraction * (hi - lo)
        state["detail"] = (f"batch {batch_index}/{total_batches}, "
                           f"{genes_done}/{total_genes} genes")
        self._put(provider, state)

    def get_all_states(self):
        return {p: json.loads(v) for p, v in self._data.items()}

    def get_overall_percentage(self):
        states = self.get_all_states()
        if not states:
            return 0.0
        return sum(s["percentage"] for s in states.values()) / len(states)


class StatusLogger(threading.Thread):
    """Daemon thread that periodically logs pipeline progress."""

    def __init__(self, tracker, interval=30):
        super().__init__(daemon=True)
        self._tracker = tracker
        self._interval = interval
        self._stop_event = threading.Event()

    def run(self):
        while not self._stop_event.wait(self._interval):
            self._log_status()

    def stop(self):
        self._stop_event.set()

    def _log_status(self):
        states = self._tracker.get_all_states()
        if not states:
            return
        lines = [
            "=" * 70,
            "  PIPELINE PROGRESS STATUS",
            "-" * 70,
        ]
        for provider in sorted(states):
            s = states[provider]
            phase = s["phase"]
            pct = s["percentage"]
            detail = s["detail"]
            if phase == ProviderPhase.DONE.value and s.get("elapsed"):
                elapsed_str = time.strftime(
                    "%H:%M:%S", time.gmtime(s["elapsed"]))
                detail = f"complete ({elapsed_str})"
            lines.append(
                f"  {provider:10s}| {phase:15s}| {pct:5.1f}% | {detail}")
        overall = self._tracker.get_overall_percentage()
        lines.append("-" * 70)
        lines.append(f"  OVERALL: {overall:.1f}%")
        lines.append("=" * 70)
        logger.info("\n" + "\n".join(lines))


def load_all_data_for_provider(data_manager: AllianceDataManager, data_provider: str, species_taxon: str,
                               progress_tracker=None):
    has_expression = data_provider in provider_to_expression_curie_prefix
    total_tasks = 5 if has_expression else 3  # GO, DO, genes [+ expr_onto, expr_annot]
    completed = 0
    _lock = threading.Lock()

    def _on_task_done(task_name):
        nonlocal completed
        with _lock:
            completed += 1
            if progress_tracker:
                progress_tracker.update_loading_progress(data_provider, completed, total_tasks, task_name)

    def _load_go():
        logger.info(f"Loading GAF file for {data_provider}")
        data_manager.load_annotations(associations_type=DataType.GO, taxon_id=species_taxon, provider=data_provider)
        _on_task_done("GO annotations loaded")

    def _load_do():
        logger.info(f"Loading disease annotations for {data_provider}")
        data_manager.load_annotations(associations_type=DataType.DO, taxon_id=species_taxon, provider=data_provider)
        _on_task_done("DO annotations loaded")

    def _load_genes():
        logger.info(f"Loading gene data for {data_provider}")
        data_manager.load_gene_data(species_taxon=species_taxon)
        _on_task_done("gene data loaded")

    def _load_expr_ontology():
        logger.info(f"Loading anatomy ontology data for {data_provider}")
        data_manager.load_ontology(ontology_type=DataType.EXPR, provider=data_provider)
        _on_task_done("expression ontology loaded")

    def _load_expr_annotations():
        logger.info(f"Loading expression annotations for {data_provider}")
        data_manager.load_annotations(associations_type=DataType.EXPR, taxon_id=species_taxon,
                                      provider=data_provider)
        _on_task_done("expression annotations loaded")

    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = [
            executor.submit(_load_go),
            executor.submit(_load_do),
            executor.submit(_load_genes),
        ]
        if has_expression:
            expr_onto_future = executor.submit(_load_expr_ontology)
            expr_onto_future.result()
            futures.append(executor.submit(_load_expr_annotations))

        for f in futures:
            f.result()

    # Prepend 'RGD:' to all gene ids if provider is HUMAN
    if data_provider == "HUMAN":
        for gene_id in list(data_manager.gene_data.keys()):
            gene = data_manager.gene_data[gene_id]
            new_gene_id = f"RGD:{gene.id}"
            data_manager.gene_data[new_gene_id] = gene._replace(id=new_gene_id)
            del data_manager.gene_data[gene_id]


def generate_gene_descriptions(data_manager: AllianceDataManager, best_orthologs, data_provider: str,
                               conf_parser: GenedescConfigParser, json_desc_writer: DescriptionsWriter,
                               gene_workers: int = 1, batch_size: int = 500,
                               progress_tracker=None):
    genes = list(data_manager.get_gene_data())
    total_genes = len(genes)

    if gene_workers > 1 and total_genes > batch_size:
        batches = [genes[i:i + batch_size]
                   for i in range(0, total_genes, batch_size)]
        total_batches = len(batches)
        logger.info(f"Processing {total_genes} genes in {total_batches} batches "
                    f"with {gene_workers} workers")
        genes_done = 0
        with concurrent.futures.ProcessPoolExecutor(
                max_workers=gene_workers,
                initializer=_init_gene_worker,
                initargs=(data_manager, best_orthologs,
                          data_provider, conf_parser)) as executor:
            for batch_idx, result_batch in enumerate(
                    executor.map(_process_gene_batch, batches), start=1):
                for gene_desc in result_batch:
                    json_desc_writer.add_gene_desc(gene_desc)
                genes_done += len(batches[batch_idx - 1])
                pct = genes_done / total_genes * 100 if total_genes else 0
                logger.info(
                    f"{data_provider}: batch {batch_idx}/{total_batches} "
                    f"complete, {genes_done}/{total_genes} genes "
                    f"({pct:.1f}%)")
                if progress_tracker:
                    progress_tracker.update_processing_progress(
                        data_provider, batch_idx, total_batches,
                        genes_done, total_genes)
    else:
        logger.info(f"Processing {total_genes} genes sequentially")
        for gene_idx, gene in enumerate(genes, start=1):
            output_gene_id = gene.id
            if data_provider == "HUMAN" and gene.id.startswith("RGD:"):
                output_gene_id = gene.id[4:]
            gene_desc = GeneDescription(gene_id=output_gene_id,
                                        gene_name=gene.name,
                                        add_gene_name=False,
                                        config=conf_parser)
            set_gene_ontology_module(dm=data_manager, conf_parser=conf_parser, gene_desc=gene_desc, gene=gene)
            if data_provider in provider_to_expression_curie_prefix:
                set_expression_module(df=data_manager,
                                      conf_parser=conf_parser,
                                      gene_desc=gene_desc,
                                      gene=gene)
            set_disease_module(df=data_manager, conf_parser=conf_parser, gene_desc=gene_desc, gene=gene,
                               human=data_provider == "HUMAN")
            if gene.id in best_orthologs:
                gene_desc.stats.set_best_orthologs = best_orthologs[gene.id][0]
                set_alliance_human_orthology_module(orthologs=best_orthologs[gene.id][0],
                                                    excluded_orthologs=best_orthologs[gene.id][1],
                                                    gene_desc=gene_desc,
                                                    config=conf_parser)
            json_desc_writer.add_gene_desc(gene_desc)
            if gene_idx % 1000 == 0 or gene_idx == total_genes:
                pct = gene_idx / total_genes * 100 if total_genes else 0
                logger.info(
                    f"{data_provider}: {gene_idx}/{total_genes} genes "
                    f"({pct:.1f}%)")
                if progress_tracker:
                    total_batches = (total_genes + 999) // 1000
                    batch_idx = (gene_idx + 999) // 1000
                    progress_tracker.update_processing_progress(
                        data_provider, batch_idx, total_batches,
                        gene_idx, total_genes)


# --- Gene-level parallel processing support ---

_worker_data = {}


def _init_gene_worker(data_manager, best_orthologs, data_provider, conf_parser):
    """Initializer for gene processing worker processes.

    Called once per worker at pool creation. Stores shared read-only data
    in a module-level global so it is available to _process_gene_batch
    without per-task pickling overhead.
    """
    _worker_data['dm'] = data_manager
    _worker_data['orthologs'] = best_orthologs
    _worker_data['provider'] = data_provider
    _worker_data['conf'] = conf_parser


def _process_gene_batch(gene_batch):
    """Process a batch of genes in a worker process.

    Returns:
        list[GeneDescription]: descriptions for all genes in the batch.
    """
    dm = _worker_data['dm']
    best_orthologs = _worker_data['orthologs']
    data_provider = _worker_data['provider']
    conf_parser = _worker_data['conf']
    results = []
    for gene in gene_batch:
        output_gene_id = gene.id
        if data_provider == "HUMAN" and gene.id.startswith("RGD:"):
            output_gene_id = gene.id[4:]
        gene_desc = GeneDescription(gene_id=output_gene_id,
                                    gene_name=gene.name,
                                    add_gene_name=False,
                                    config=conf_parser)
        set_gene_ontology_module(dm=dm, conf_parser=conf_parser,
                                 gene_desc=gene_desc, gene=gene)
        if data_provider in provider_to_expression_curie_prefix:
            set_expression_module(df=dm, conf_parser=conf_parser,
                                  gene_desc=gene_desc, gene=gene)
        set_disease_module(df=dm, conf_parser=conf_parser,
                           gene_desc=gene_desc, gene=gene,
                           human=data_provider == "HUMAN")
        if gene.id in best_orthologs:
            gene_desc.stats.set_best_orthologs = best_orthologs[gene.id][0]
            set_alliance_human_orthology_module(
                orthologs=best_orthologs[gene.id][0],
                excluded_orthologs=best_orthologs[gene.id][1],
                gene_desc=gene_desc, config=conf_parser)
        results.append(gene_desc)
    return results


def add_header_to_file(file_path: str, data_format: str, data_provider: str):
    """Prepend a standard Alliance header to a generated file."""
    alliance_release = os.environ.get("ALLIANCE_RELEASE", "")
    header = FILE_HEADER_TEMPLATE.substitute(
        file_type="Gene Descriptions",
        data_format=data_format,
        readme=FILE_README,
        taxon_id=TAXON_BY_PROVIDER.get(data_provider, ""),
        species=SPECIES_BY_PROVIDER.get(data_provider, ""),
        database_version=alliance_release,
        gen_time=datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M")
    )
    with open(file_path, "r") as original:
        data = original.read()
    with open(file_path, "w") as modified:
        modified.write(header + "\n\n" + data)


def _build_file_header(data_format: str, data_provider: str) -> str:
    """Build the standard Alliance file header string."""
    alliance_release = os.environ.get("ALLIANCE_RELEASE", "")
    return FILE_HEADER_TEMPLATE.substitute(
        file_type="Gene Descriptions",
        data_format=data_format,
        readme=FILE_README,
        taxon_id=TAXON_BY_PROVIDER.get(data_provider, ""),
        species=SPECIES_BY_PROVIDER.get(data_provider, ""),
        database_version=alliance_release,
        gen_time=datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M")
    )


def save_gene_descriptions(data_manager: AllianceDataManager, json_desc_writer: DescriptionsWriter,
                           data_provider: str, progress_tracker=None):
    base_path = f"pipelines/alliance/generated_descriptions/{data_provider}"
    alliance_release = os.environ.get("ALLIANCE_RELEASE", "")
    release_version = ".".join(alliance_release.split(".")[0:2])

    json_desc_writer.overall_properties.species = data_provider
    json_desc_writer.overall_properties.release_version = release_version
    json_desc_writer.overall_properties.date = datetime.date.today().isoformat()

    if progress_tracker:
        progress_tracker.set_phase(data_provider, ProviderPhase.WRITING_JSON,
                                   detail="writing JSON")
    json_desc_writer.write_json(file_path=base_path + ".json",
                                include_single_gene_stats=True,
                                data_manager=data_manager)

    if progress_tracker:
        progress_tracker.set_phase(data_provider, ProviderPhase.WRITING_FILES,
                                   detail="writing TSV/TXT")
    json_desc_writer.write_tsv(file_path=base_path + ".tsv",
                               header=_build_file_header("tsv", data_provider))
    json_desc_writer.write_plain_text(file_path=base_path + ".txt",
                                      header=_build_file_header("txt", data_provider))

    with open(base_path + "_stats.json", "w") as stats_file:
        json.dump(vars(json_desc_writer.general_stats), stats_file)
    logger.info(f"Saved description files for {data_provider}")

    if progress_tracker:
        progress_tracker.set_phase(data_provider, ProviderPhase.WRITING_DB,
                                   detail="writing to database")
    gene_desc_pairs = [
        (gd.gene_id, gd.description)
        for gd in json_desc_writer.data if gd.description
    ]
    data_manager.write_gene_description_notes(gene_desc_pairs)
    logger.info(f"Wrote gene description notes to database for {data_provider}")


def process_provider(data_provider, species_taxon, data_manager, conf_parser,
                     gene_workers=1, batch_size=500, progress_tracker=None):
    try:
        logger.info(f"Processing provider: {data_provider}")
        provider_start = time.time()
        json_desc_writer = DescriptionsWriter()

        if progress_tracker:
            progress_tracker.set_phase(data_provider, ProviderPhase.LOADING_DATA)

        logger.info(f"Loading all data for {data_provider}")
        load_all_data_for_provider(data_manager, data_provider, species_taxon,
                                   progress_tracker=progress_tracker)

        best_orthologs = {}
        if data_provider != "HUMAN":
            if progress_tracker:
                progress_tracker.set_phase(data_provider, ProviderPhase.LOADING_ORTHOLOGS,
                                           detail="loading orthologs")
            logger.info(f"Loading best human orthologs for {data_provider}")
            best_orthologs = data_manager.get_best_human_orthologs(species_taxon=species_taxon)

        total_genes = len(list(data_manager.get_gene_data()))
        if progress_tracker:
            progress_tracker.set_phase(
                data_provider, ProviderPhase.PROCESSING,
                total_genes=total_genes)

        logger.info(f"Generating text summaries for {data_provider}")
        generate_gene_descriptions(data_manager, best_orthologs, data_provider, conf_parser, json_desc_writer,
                                   gene_workers=gene_workers, batch_size=batch_size,
                                   progress_tracker=progress_tracker)

        logger.info(f"Saving gene descriptions for {data_provider}")
        save_gene_descriptions(data_manager, json_desc_writer, data_provider,
                               progress_tracker=progress_tracker)

        if progress_tracker:
            progress_tracker.set_phase(data_provider, ProviderPhase.DONE)

        elapsed = time.time() - provider_start
        formatted = time.strftime("%H:%M:%S", time.gmtime(elapsed))
        logger.info(f"===== {data_provider} DONE in {formatted} =====")
        return data_provider, elapsed
    except Exception:
        if progress_tracker:
            progress_tracker.set_phase(
                data_provider, ProviderPhase.ERROR,
                detail=traceback.format_exc().splitlines()[-1])
        raise


def main():
    parser = argparse.ArgumentParser(description="Generate gene descriptions for wormbase")
    parser.add_argument("-c", "--config-file", metavar="config_file", dest="config_file", type=str,
                        default="config.yml", help="configuration file. Default ./config.yaml")
    parser.add_argument("-L", "--log-level", dest="log_level",
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], default="INFO",
                        help="set the logging level")
    parser.add_argument("--parallel-providers", dest="parallel_providers", action="store_true",
                        help="Process data providers (MODs) in parallel")
    parser.add_argument("--provider-workers", dest="provider_workers", type=int, default=None,
                        help="Number of workers for provider-level parallelism (default: number of CPUs)")
    parser.add_argument("--parallel-genes", dest="parallel_genes", action="store_true",
                        help="Process genes within each provider in parallel batches")
    parser.add_argument("--gene-workers", dest="gene_workers", type=int, default=None,
                        help="Number of workers for gene-level parallelism (default: number of CPUs)")
    parser.add_argument("--batch-size", dest="batch_size", type=int, default=500,
                        help="Number of genes per batch for gene-level parallelism (default: 500)")
    parser.add_argument("--status-interval", dest="status_interval", type=int, default=30,
                        help="Seconds between progress status logs (default: 30, 0 to disable)")

    args = parser.parse_args()
    logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s: %(message)s')
    logging.getLogger(__name__).setLevel(logging.getLevelName(args.log_level))

    # Check required environment variables
    required_vars = [
        "PERSISTENT_STORE_DB_USERNAME",
        "PERSISTENT_STORE_DB_PASSWORD",
        "PERSISTENT_STORE_DB_NAME",
        "PERSISTENT_STORE_DB_HOST",
        "ALLIANCE_RELEASE",
    ]
    optional_vars_defaults = {
        "PERSISTENT_STORE_DB_PORT": "5432",
        "FMS_API_URL": "https://fms.alliancegenome.org",
        "API_KEY": "",
    }
    missing = [v for v in required_vars if not os.environ.get(v)]
    if missing:
        logger.error(f"Missing required environment variables: "
                     f"{', '.join(missing)}")
        raise SystemExit(1)

    logger.info("Environment variables:")
    for var in required_vars:
        logger.info(f"  {var}={os.environ.get(var, '')}")
    for var, default in optional_vars_defaults.items():
        value = os.environ.get(var, "")
        if value:
            display = value
        else:
            display = f"{default} (default)" if default else "(not set)"
        logger.info(f"  {var}={display}")

    install_ontology_cache()

    start_time = time.time()
    conf_parser = GenedescConfigParser(args.config_file)

    data_manager = AllianceDataManager(config=conf_parser)

    logger.info("Loading data providers")
    data_providers = data_manager.load_data_providers()

    logger.info("Loading GO ontology")
    data_manager.load_ontology(ontology_type=DataType.GO)

    logger.info("Loading DO ontology")
    data_manager.load_ontology(ontology_type=DataType.DO)

    logger.info("Deleting existing automated gene descriptions")
    data_manager.delete_all_automated_gene_descriptions()

    # Close DB connection before spawning subprocesses - each subprocess will create its own
    # This is necessary because SQLAlchemy connections can't be pickled
    data_manager.close()

    if args.parallel_genes:
        gene_workers = args.gene_workers if args.gene_workers is not None else os.cpu_count()
        batch_size = args.batch_size
        logger.info(f"Gene-level parallelism: {gene_workers} workers, batch size {batch_size}")
    else:
        gene_workers = 1
        batch_size = args.batch_size

    provider_names = [dp for dp, _ in data_providers]
    progress_tracker = ProgressTracker(
        provider_names, shared=args.parallel_providers)

    status_logger = None
    if args.status_interval > 0:
        status_logger = StatusLogger(
            progress_tracker, interval=args.status_interval)
        status_logger.start()

    provider_times = {}

    try:
        if args.parallel_providers:
            logger.info("Processing data providers in parallel")
            with concurrent.futures.ProcessPoolExecutor(max_workers=args.provider_workers) as executor:
                future_to_provider = {
                    executor.submit(process_provider, data_provider, species_taxon, data_manager, conf_parser,
                                    gene_workers=gene_workers, batch_size=batch_size,
                                    progress_tracker=progress_tracker): data_provider
                    for data_provider, species_taxon in data_providers
                }
                for future in concurrent.futures.as_completed(future_to_provider):
                    dp = future_to_provider[future]
                    try:
                        provider, elapsed = future.result()
                        provider_times[provider] = elapsed
                    except Exception as e:
                        logger.error(f"Error processing data provider: {e}")
                        logger.error(traceback.format_exc())
                        progress_tracker.set_phase(
                            dp, ProviderPhase.ERROR, detail=str(e))
        else:
            logger.info("Processing data providers sequentially")
            for data_provider, species_taxon in data_providers:
                provider, elapsed = process_provider(
                    data_provider, species_taxon, data_manager, conf_parser,
                    gene_workers=gene_workers, batch_size=batch_size,
                    progress_tracker=progress_tracker
                )
                provider_times[provider] = elapsed
    finally:
        if status_logger:
            status_logger.stop()

    elapsed_time = time.time() - start_time
    formatted_time = time.strftime("%H:%M:%S", time.gmtime(elapsed_time))

    logger.info("=" * 50)
    logger.info("Processing times per provider:")
    for provider, elapsed in sorted(provider_times.items(),
                                    key=lambda x: x[1], reverse=True):
        formatted = time.strftime("%H:%M:%S", time.gmtime(elapsed))
        logger.info(f"  {provider:10s} {formatted}")
    logger.info(f"Total pipeline time: {formatted_time}")
    logger.info("=" * 50)


if __name__ == '__main__':
    main()
